---
layout: default
title: "TeraSort on a 100Gb/s network using Spark/Crail (DRAFT)"
author: Patrick Stuedi
category: blog
---

In this blog we explore the sorting performance of Spark/Crail on a 100Gbit/s RDMA cluster. Sorting large data sets efficiently on a cluster is particularly interesting from a network perspective as most of the input data will have to cross the network at least once. Hence, a TeraSort workload should be an ideal candidate to be accelerated by a fast network. 

### Hardware Configuration

The specific cluster configuration used for the experiments in this blog:

* Cluster
  * 128 node OpenPower cluster
* Node configuration
  * CPU: 2x OpenPOWER Power8 10-core @2.9Ghz
  * DRAM: 512GB DDR4
  * Storage: 4x Huawei ES3600P V3 1.2TB NVMe SSD
  * Network: 100Gbit/s Ethernet Mellanox ConnectX-4 EN (RoCE)
* Software
  * Ubuntu 16.04 with Linux kernel version 4.4.0-31
  * Spark 2.0.0
  * Crail 1.0 (Crail only used during shuffle, input/output is on HDFS)

### Anatomy Spark TeraSort

A Spark sorting job  run consists of two phases. The first phase is a mapping or classification phase - where individual workers
read their part of the key-value (KV) input data and classify the KV pairs based on their keys. This phase involves only very little networking as all workers run locally on the nodes that host the input HDFS data blocks. During the second so called reduce phase, each worker collects all KV pairs from all workers for a particular key range, and then sorts the data. This pipeline runs on all cores in multiple waves of tasks on all the compute nodes. Naturally, the performance of such a pipeline depends upon both the network as well as the CPU performance, which together should dictate the overall job run time.

<br>
<div style="text-align:center"><img src ="http://crail.io/img/blog/sort/terasort_pipeline.png" width="450"></div>
<br><br>

<br>
<img src="http://crail.io/img/blog/sort/terasort_pipeline.png" width="550" align="middle">
<br><br>

### Using Vanilla Spark

The first question we are interested in is to what extent Spark can drive the 100Gbit/s network fabric. Making good use of the network is important since a reduce task needs to first fetch all the relevant data from the network before it can start sorting the data. Unfortunately, it turns out that when running vanilla Spark on the cluster, the network usage stays at only 5-10%. 

<br>
<div style="text-align:center"><img src ="http://crail.io/docs/net_vanilla.svg" /></div>
<br><br>

### Using Crail Shuffler

<div style="text-align: justify"> 
<p>
Retaining the hardware performance in a distributed setting where DRAM and flash resources are accessed over a fast network is challenging. The figure below shows the network throughput of a shuffle operation in a simple Spark sorting workload on a 100Gb/s cluster. Note that during this mapreduce job, a reduce task needs to first fetch all the relevant data from the network before it can start sorting the data. Despite the urgent need to fetch data as fast as possible, the network usage stays at only 5-10%. 
</p>
</div>



<div style="text-align: justify">
<p>
In fact, making good use of a high-speed network is challenging for many of the prominent data processing frameworks and workloads. While commonly a network upgrade from 1Gb/s to 10Gb/s leads to a reduction of the application runtime, further network upgrades to 40Gb/s or even 100Gb/s (not shown) do not translate into performance improvements at all.
</p>
</div>

<img src="http://crail.io/docs/net_apache.png" width="600">
