---
layout: default
title: "Sorting on a 100Gbit/s Cluster using Spark/Crail (DRAFT)"
author: Patrick Stuedi
category: blog
---

<div style="text-align: justify"> 
<p>
In this blog we explore the sorting performance of Spark/Crail on a 100Gbit/s RDMA cluster. Sorting large data sets efficiently on a cluster is particularly interesting from a network perspective as most of the input data will have to cross the network at least once. Hence, a TeraSort workload should be an ideal candidate to be accelerated by a fast network. 
</p>
<p>
The following table summarizes the results of this blog and provides a comparison with other sorting benchmarks. In essence, Spark/Crail is sorting 12.8 TB of data in 98 seconds, which calculates to a sorting rate per code of 3.13 GB/min. This is about a factor of 5 faster than the sorting performance of the <a href="https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html">Spark 2014 benchmark winner</a>, and only about 28% slower than the <a href="http://sortbenchmark.org/TencentSort2016.pdf">2016 winner of the sorting benchmark</a> -- a sorting benchmark running natively dedicated to sorting only. 
</p>
</div>
<br>

<center>
|                    | Spark/Crail   | Spark/Vanilla |    Spark/Winner2014    | Tencent/Winner2016       |
|--------------------|---------------|---------------|------------------------|--------------------------|
| Data Size          |   12.8 TB     |      12.8 TB  |         100 TB         |            100 TB        |
| Elapsed Time       |    98s        |       527s    |        1406s           |         98.8s            |
| Cores              |     2560      |     2560      |        6592            |         10240            |
| Nodes              |    128        |   128         |         206            |           512            |
| Network            |   100Gbit/s   |  100Gbit/s    |       10Gbit/s         |       100Gbit/s          |
| Sorting rate       |  7.8 TB/min   |  1.4 TB/min   |      4.27 TB/min       |         44.78 TB/min     |
| Sorting rate/core  |  3.13 GB/min  |  0.58 GB/min  |      0.66 GB/min       |          4.4 GB/min      |
</center>

### Hardware Configuration

The specific cluster configuration used for the experiments in this blog:

* Cluster
  * 128 node OpenPower cluster
* Node configuration
  * CPU: 2x OpenPOWER Power8 10-core @2.9Ghz
  * DRAM: 512GB DDR4
  * Storage: 4x Huawei ES3600P V3 1.2TB NVMe SSD
  * Network: 100Gbit/s Ethernet Mellanox ConnectX-4 EN (RoCE)
* Software
  * Ubuntu 16.04 with Linux kernel version 4.4.0-31
  * Spark 2.0.0
  * Crail 1.0 (Crail only used during shuffle, input/output is on HDFS)

### Anatomy Spark TeraSort

<div style="text-align: justify"> 
<p>
A Spark sorting job  run consists of two phases. The first phase is a mapping or classification phase - where individual workers
read their part of the key-value (KV) input data and classify the KV pairs based on their keys. This phase involves only very little networking as all workers run locally on the nodes that host the input HDFS data blocks. During the second so called reduce phase, each worker collects all KV pairs from all workers for a particular key range, de-serializes the data into objects and sorts them. This pipeline runs on all cores in multiple waves of tasks on all the compute nodes. Naturally, the performance of such a pipeline depends upon both the network as well as the CPU performance, which together should dictate the overall job run time.
</p>
</div>

<br>
<div style="text-align:center"><img src ="http://crail.io/img/blog/sort/terasort_pipeline.png" width="490"></div>
<br><br>

### Using Vanilla Spark

<div style="text-align: justify"> 
<p>
The first question we are interested in is to what extent Spark can drive the 100Gbit/s network fabric. Making good use of the network is important since a reduce task needs to first fetch all the relevant data from the network before it can start sorting the data. Unfortunately, it turns out that when running vanilla Spark on the cluster, the network usage stays at only 5-10%. 
</p>
</div>

<br>
<div style="text-align:center"><img src ="http://crail.io/docs/net_vanilla.svg" /></div>
<br><br>

<div style="text-align: justify"> 
<p>
The poor network usage matches with the general observation we made in our previous [HotCloud'16 publication](http://dl.acm.org/citation.cfm?id=3027062) where we show that a faster network does not necessarily lead to a better runtime. The problem can be understood by looking at a single reduce task in TeraSort. While the actual time for fetching all the data over the network decreases with increasing networking bandwidth, the time a reduce task spends on funneling the data through the stack, deserializing and sorting it outweighs the transmission time by far. The figure below shows percentage wise for different network technologies, how much time a TeraSort reduce task spends on waiting for data versus executing CPU instructions (a more detailed breakdown of the time can be found in the [paper](http://dl.acm.org/citation.cfm?id=3027062)). Clearly, in such a situation, more network bandwidth will only improve the increasinlgy small red part and, thus, will not result in a overall runtime reduction. 
</p>
</div>

<br>
<div style="text-align:center"><img src ="http://crail.io/img/blog/sort/cpu_network.svg"/></div>
<br><br>

<div style="text-align: justify"> 
<p>
Consequently, to improve the runtime of TeraSort and to levarage the 100Gbit/s network, the amount of CPU instructions executed by byte transferred need to be reduce massively. In the following, we show how we use the Crail shuffle engine to cut down the software overheads related to networking, deserialization and sorting and thereby empower TeraSort reduce tasks to consume data at the speed very close the network limit (~80Gbit/s). 
</p>
</div>

### Using Crail Shuffler

While in the built-in Spark shuffler, many CPU instructions are necessary to bring the data from the networking interface to the buffer inside Spark, with the Crail shuffler very little instructions required as data is placed directly from the network interface to the shuffle buffer using DMA. 
