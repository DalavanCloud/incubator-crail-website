---
layout: default
title: "Sorting on a 100Gbit/s Cluster using Spark/Crail (DRAFT)"
author: Patrick Stuedi
category: blog
---

<div style="text-align: justify"> 
<p>
In this blog we explore the sorting performance of Spark/Crail on a 100Gbit/s RDMA cluster. Sorting large data sets efficiently on a cluster is particularly interesting from a network perspective as most of the input data will have to cross the network at least once. Hence, a TeraSort workload should be an ideal candidate to be accelerated by a fast network. 
</p>
<p>
The following table summarizes the results of this blog and provides a comparison with other sorting benchmarks. In essence, Spark/Crail is sorting 12.8 TB of data in 98 seconds, which calculates to a sorting rate per code of 3.13 GB/min. This is about a factor of 5 faster than the sorting performance of the <a href="https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html">Spark 2014 benchmark winner</a>, and only about 28% slower than the <a href="http://sortbenchmark.org/TencentSort2016.pdf">2016 winner of the sorting benchmark</a> -- a sorting benchmark running natively dedicated to sorting only. 
</p>
</div>
<br>

<center>
<table>
  <thead>
    <tr>
      <th> </th>
      <th>Spark/Crail</th>
      <th>Spark/Vanilla</th>
      <th>Spark/Winner2014</th>
      <th>Tencent/Winner2016</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Data Size</td>
      <td>12.8 TB</td>
      <td>12.8 TB</td>
      <td>100 TB</td>
      <td>100 TB</td>
    </tr>
    <tr>
      <td>Elapsed Time</td>
      <td>98s</td>
      <td>527s</td>
      <td>1406s</td>
      <td>98.8s</td>
    </tr>
    <tr>
      <td>Cores</td>
      <td>2560</td>
      <td>2560</td>
      <td>6592</td>
      <td>10240</td>
    </tr>
    <tr>
      <td>Nodes</td>
      <td>128</td>
      <td>128</td>
      <td>206</td>
      <td>512</td>
    </tr>
    <tr>
      <td>Network</td>
      <td>100Gbit/s</td>
      <td>100Gbit/s</td>
      <td>10Gbit/s</td>
      <td>100Gbit/s</td>
    </tr>
    <tr>
      <td>Sorting rate</td>
      <td>7.8 TB/min</td>
      <td>1.4 TB/min</td>
      <td>4.27 TB/min</td>
      <td>44.78 TB/min</td>
    </tr>
    <tr>
      <td>Sorting rate/core</td>
      <td>3.13 GB/min</td>
      <td>0.58 GB/min</td>
      <td>0.66 GB/min</td>
      <td>4.4 GB/min</td>
    </tr>
  </tbody>
</table>
</center>

<br>

### Hardware Configuration

The specific cluster configuration used for the experiments in this blog:

* Cluster
  * 128 node OpenPower cluster
* Node configuration
  * CPU: 2x OpenPOWER Power8 10-core @2.9Ghz
  * DRAM: 512GB DDR4
  * Storage: 4x Huawei ES3600P V3 1.2TB NVMe SSD
  * Network: 100Gbit/s Ethernet Mellanox ConnectX-4 EN (RoCE)
* Software
  * Ubuntu 16.04 with Linux kernel version 4.4.0-31
  * Spark 2.0.0
  * Crail 1.0 (Crail only used during shuffle, input/output is on HDFS)

### Anatomy Spark TeraSort

<div style="text-align: justify"> 
<p>
A Spark sorting job  run consists of two phases. The first phase is a mapping or classification phase - where individual workers
read their part of the key-value (KV) input data and classify the KV pairs based on their keys. This phase involves only very little networking as all workers run locally on the nodes that host the input HDFS data blocks. During the second so called reduce phase, each worker collects all KV pairs from all workers for a particular key range, de-serializes the data into objects and sorts them. This pipeline runs on all cores in multiple waves of tasks on all the compute nodes. Naturally, the performance of such a pipeline depends upon both the network as well as the CPU performance, which together should dictate the overall job run time.
</p>
</div>

<br>
<div style="text-align:center"><img src ="http://crail.io/img/blog/sort/terasort_pipeline.png" width="490"></div>
<br><br>

### Using Vanilla Spark

<div style="text-align: justify"> 
<p>
The first question we are interested in is to what extent Spark can drive the 100Gbit/s network fabric. Making good use of the network is important since a reduce task needs to first fetch all the relevant data from the network before it can start sorting the data. Unfortunately, it turns out that when running vanilla Spark on the cluster, the network usage stays at only 5-10%. 
</p>
</div>

<br>
<div style="text-align:center"><img src ="http://crail.io/docs/net_vanilla.svg" /></div>
<br><br>

<div style="text-align: justify"> 
<p>
The poor network usage matches with the general observation we made in our previous <a href="http://dl.acm.org/citation.cfm?id=3027062">HotCloud'16 publication</a> where we show that a faster network does not necessarily lead to a better runtime. The problem can be understood by looking at a single reduce task in TeraSort. While the actual time for fetching all the data over the network decreases with increasing networking bandwidth, the time a reduce task spends on funneling the data through the stack, deserializing and sorting it outweighs the transmission time by far. The figure below shows percentage wise for different network technologies, how much time a TeraSort reduce task spends on waiting for data versus executing CPU instructions (a more detailed breakdown of the time can be found in the <a href="http://dl.acm.org/citation.cfm?id=3027062">paper</a>. Clearly, in such a situation, more network bandwidth will only improve the increasinlgy small red part and, thus, will not result in a overall runtime reduction. 
</p>
</div>

<br>
<div style="text-align:center"><img src ="http://crail.io/img/blog/sort/cpu_network.svg"/></div>
<br><br>

<div style="text-align: justify"> 
<p>
Consequently, to improve the runtime of TeraSort and to levarage the 100Gbit/s network, the amount of CPU instructions executed by byte transferred need to be reduce massively. In the following, we show how we use the Crail shuffle engine to cut down the software overheads related to networking, deserialization and sorting and thereby empower TeraSort reduce tasks to consume data at the speed very close the network limit (~80Gbit/s). 
</p>
</div>

### Using Crail Shuffler

<div style="text-align: justify"> 
<p>
An overview of the Crail shuffler is provided in the <a href="http://www.crail.io/docs">documentatin section</a>. The main difference between the Crail shuffler and the Spark built-in shuffler lies in the way data from the network is processed in a reduce task. The Spark shuffler is based on TPC sockets, thus, many CPU instructions are necessary to bring the data from the networking interface to the buffer inside Spark. In contrast, the Crail shuffler shares shuffle data through the Crail file system, and consequently data is placed directly from the network interface into the Spark shuffle buffer using DMA, which is much more efficient. 
</p>
</div>

<br>
<div style="text-align:center"><img src ="http://crail.io/img/blog/sort/shuffle_rdma.png" width="470"></div>
<br><br>

<div style="text-align: justify"> 
<p>
The Crail shuffler during the map phase organizes each key range in a set of Crail files, one file per Spark core per key range. Reading the shuffle data from the Crail file system during in the reduce phase not only eliminates data copies and avoids system calls, it also makes sure the different segments (files) are placed at the right offset within the reduce buffer to create one contiguous memory area that can immediately be used for deserialization and sorting.  
</p>

<p>
As pointed out in the <a href="http://www.crail.io/docs">documentatin section</a>, the Crail shuffler allows applications to pass their own custom serializer and sorter. Spark by default uses the Kryo serializer, which is a generic serializer. Being generic, however, comes at a cost. Specifically, Kryo requires more type information to be stored along with the serialized data than a custom serializer, and also the parsing is more comples. On top of this, Kryo also comes with its own buffering, introducing additional memory copies. In our benchmark, we use a custom serializer that takes advantage of the fact that the data consists of fixed size key/value pairs. The custom serializer further avoids extra buffering and directly interfaces with the Crail streams when reading and writing data. 
</p>
<p>
As with serialization, the Spark built-in sorter is a general purpose TimSort that can sort arbitrary comparable objects. In our benchmark, we instruct the Crail shuffler to use a Radix sorter instead. The Radix sorter cannot be applied to arbitrary objects but works well for keys of a fixed byte length. The standard pipeline of a reduce task is to first deserialize the data and then sort it. In this particular configuration of the Crail shuffler, we turn these two steps around and first sort the data and deserialize it later. This is possible because the data is read into a contiguous off-heap buffer that can be sorted almost in-place. 
</p>
</div>

### Spark/Crail Sorting Performance

<div style="text-align: justify"> 
<p>
The figure below shows the overall performance of Spark/Crail vs Spark/Vanilla in a 12.8 TB sorting benchmark. With a cluster size of 128 nodes, each node gets to sort 100GB if data is distributed evenly. As can be seen, using the Crail shuffler, the total job runtime is reduced by a factor of 6. Most of the gains come from the reduce side, which is also where the networking takes place. However, also the map phase is faster which comes from the more efficient serialization but also from a faster I/O. The built-in Spark shuffler dumps data into files absorbed by the buffer cache (no disk writes took place during shuffle), which requires file system calls and data copies. The Crail shuffler instead uses memory mapped I/O to write local Crail files, avoiding both data copies and system calls. 
</p>
</div>

<br>
<div style="text-align:center"><img src ="http://crail.io/img/blog/sort/performance_overall.png" width="470"></div>
<br><br>

One 
