---
layout: default
title: "TeraSort on a 100Gb/s network using Spark/Crail (DRAFT)"
author: someone
category: blog
---

In this blog we explore the sorting performance of Spark/Crail on a 128 node cluster equipped with a 100Gbit/s RDMA-capable Ethernet network. Sorting large data sets efficiently on a cluster is particularly interesting from a network perspective as most of the input data will have to cross the network at least once. Hence, a TeraSort workload should be an ideal candidate to be accelerated by a fast network. 

### Hardware Configuration

The specific cluster configuration used for the experiments in this blog:

* Cluster
  * 128 node OpenPower Cluster
* Node configuration
  * CPU: 2x OpenPOWER Power8 10-core @2.9Ghz

### Anatomy Spark TeraSort

A Spark sorting job  run consists of two phases. The first phase is a mapping or classification phase - where individual workers
read their part of the key-value (KV) input data and classify the KV pairs based on their keys. This phase involves only very little networking as all workers run locally on the nodes that host the input HDFS data blocks. During the second so called reduce phase, each worker collects all KV pairs from all workers for a particular key range, and then sorts the data.

To understand Spark's processing cost we profile its CPU usage. Figure~\ref{fig:data-ingestion-pipline} shows a simplistic view of TeraSort which helps us understand Spark's CPU requirements, especially for the reduce phase of TeraSort. The data ingestion pipeline of the reduce phase involves scheduling, fetching data over the network, and finally sorting. This pipeline runs on all cores in multiple waves of tasks on all the compute nodes. In the follow up discussions we calculate the \textit{data-ingestion} bandwidth of this pipeline for various settings. Naturally, the performance of such a pipeline depends upon both the network as well as the CPU performance, which together \textit{should} dictate the overall job run time.

![vanilla_net](http://crail.io/docs/net_vanilla.svg)

### Using Crail Shuffler

<div style="text-align: justify"> 
<p>
Retaining the hardware performance in a distributed setting where DRAM and flash resources are accessed over a fast network is challenging. The figure below shows the network throughput of a shuffle operation in a simple Spark sorting workload on a 100Gb/s cluster. Note that during this mapreduce job, a reduce task needs to first fetch all the relevant data from the network before it can start sorting the data. Despite the urgent need to fetch data as fast as possible, the network usage stays at only 5-10%. 
</p>
</div>

![vanilla_net](http://crail.io/docs/net_vanilla.svg)

<div style="text-align: justify">
<p>
In fact, making good use of a high-speed network is challenging for many of the prominent data processing frameworks and workloads. While commonly a network upgrade from 1Gb/s to 10Gb/s leads to a reduction of the application runtime, further network upgrades to 40Gb/s or even 100Gb/s (not shown) do not translate into performance improvements at all.
</p>
</div>

<img src="http://crail.io/docs/net_apache.png" width="600">
